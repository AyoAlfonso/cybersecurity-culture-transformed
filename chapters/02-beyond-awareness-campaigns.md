# Chapter 2: Beyond Awareness Campaigns – Building for Behavioural change

## **2.0 The Empty Feast: When Awareness is Not Nourishment**

Imagine a great feast laid out for the village. The tables groan under the weight of beautifully presented dishes, each one a perfect visual spectacle. You are shown every delicacy, given its name, and told its importance. Yet, when you reach out to eat, you are given only a picture of the food. The spectacle is vast, but it provides no sustenance. Your body remains empty, your hunger unfulfilled. The feast is a hollow performance, a ritual that acknowledges the concept of nourishment but fails to deliver its essence.

For decades, this has been the prevailing model of cybersecurity awareness. Organisations have invested immense resources in crafting the spectacle: annual mandatory training modules that employees click through in a mindless rush; glossy posters warning of ‘danger\!’ in the breakroom; fear-laden emails from the IT department. We have become masters of the awareness campaign, yet the hunger for genuine security remains. We have shown our people the menu of threats but have failed to provide them with the tools, the skills, or the motivation to actually change their behaviour. We have hosted an empty feast.

The result is what scholars and practitioners call the ‘awareness-behaviour gap’—the profound chasm between knowing what to do and actually doing it (Hadnagy, 2018). An employee can perfectly recite the policy against password reuse and still use the same simple password across a dozen personal and professional accounts. They can pass a phishing test with flying colours and still click a malicious link in a moment of stress or distraction. This gap is not a sign of employee failure; it is a definitive indictment of an awareness-only approach. It proves that simply informing people is a woefully inadequate strategy for inspiring lasting change.

This chapter moves beyond the empty spectacle. We will dissect why these traditional methods, while well-intentioned, are fundamentally designed to fail. We will explore the psychological mechanics behind why fear and compliance are poor motivators for secure behaviour, often breeding resentment and workarounds instead of vigilance. Most importantly, we will lay the foundation for a new model—one rooted in behavioural science, continuous engagement, and tangible empowerment. This is not about serving more pictures of food. It is about rebuilding the kitchen, teaching everyone to cook, and creating a culture where secure behaviour becomes the most natural, satisfying, and nourishing choice.

References

Hadnagy, C. (2018). Social Engineering: The Science of Human Hacking (2nd ed.). John Wiley & Sons.

PCIe (2023). The Human Factor in Cybersecurity: Exploratory Findings. PCIe Publications.

## **2.1. The Path of Knowing, The Path of Doing: Bridging the Great Divide**

An elder knows the precise path to the life-giving river. They can describe every turning, every landmark tree, every patch of unstable ground in intricate detail. They can draw the map. The child, having received this knowledge, must now walk the path. Yet, on the journey, the child’s attention is caught by a colourful bird. The heat of the sun makes them weary. A fallen tree blocks the known way, forcing a detour through unfamiliar terrain. The map is perfect, but the journey is imperfect. The child knows the path, but in the moment of action, that knowledge is competing with a thousand other pressures, distractions, and urges. Knowing the path and walking the path are two fundamentally different challenges.

This is the great divide in cybersecurity: the chasm between awareness and behaviour. We have become expert cartographers, drawing ever more detailed maps in the form of policies, training modules, and awareness campaigns. We have told our people where the dangers are. And yet, breaches born from human action continue to dominate. This is not because people are inherently negligent or incapable of learning. It is because we have fundamentally misunderstood the nature of the problem. We have treated a behavioural challenge as an information deficit problem. We assumed that providing knowledge—the map—would be enough to ensure safe passage. Behavioural science reveals this to be a critical error.

This chapter dissects the ‘awareness-behaviour gap,’ moving beyond the frustration of ‘they should know better’ to a scientific understanding of ‘why don’t they *do* better?’ We will explore the powerful cognitive and environmental forces that cause intention to crumble at the moment of action. Only by understanding this gap can we begin to build the bridges that will finally connect knowing to doing.

**Why Good Intentions Are Not Enough**

The traditional awareness model is built on a foundation of rational choice theory: the idea that individuals will make logical decisions that maximise their benefit and minimise their risk if given the correct information. Cybersecurity, in this view, is a simple calculation: the minor inconvenience of a secure action (creating a strong password) is outweighed by the significant benefit of avoiding a catastrophic breach.

This model is elegantly logical and empirically wrong. Human decision-making is not a cold, rational processor of information. It is a messy, emotional, and cognitive struggle between two systems, famously articulated by Nobel laureate Daniel Kahneman (2011) as System 1 and System 2\.

* **System 1** is our fast, automatic, and intuitive mind. It operates on heuristics (mental shortcuts), is driven by emotion, and is focused on conserving cognitive energy. It is the system that is active when we are tired, stressed, distracted, or rushing to meet a deadline.  
* **System 2** is our slow, analytical, and deliberate mind. It is the system that attentively completes security training, understands the logic behind multi-factor authentication, and sincerely intends to follow security protocols. This is the system most awareness programs target.

The critical insight is that **System 1 is in charge most of the time**, especially in a modern work environment characterised by constant interruptions and information overload. The great divide opens when a System 2 intention (“I must create a strong password”) is overruled by a System 1 impulse (“I need to access this file now; I’ll just use my old password one more time”). The employee is not stupid or malicious; they are human. Their automatic mind prioritised immediate task completion over a nebulous, future security risk.

**Cognitive Biases That Widen the Gap**

System 1 relies on cognitive biases to make quick judgments. These mental shortcuts are essential for navigating daily life, but they are systematically exploited by attackers and represent major obstacles to secure behaviour.

* **Optimism Bias:** “It won’t happen to me.” This is perhaps the most significant barrier to secure behaviour. People consistently believe they are less likely than others to experience negative events, such as falling for a phishing scam or downloading malware (Sharot, 2011). Awareness campaigns that focus on generalised threats inadvertently reinforce this bias. The employee acknowledges the threat exists but concludes it is a problem for others, not for them.  
* **Present Bias:** We are hardwired to value immediate rewards over future gains. The reward for creating a strong password is abstract and delayed (avoiding a future breach). The cost is immediate and tangible (time and effort spent creating and remembering the password). The immediate cost almost always wins. Similarly, the immediate reward of clicking a link to see a “delivery notification” outweighs the abstract, future risk of a breach.  
* **Confirmation Bias:** We seek out information that confirms our existing beliefs and ignore information that contradicts them. An employee who believes “the IT department is just being paranoid” will dismiss security warnings as unnecessary hurdles, selectively remembering the times they ignored a warning with no ill effects while forgetting the near-misses that were stopped by those very controls.  
* **The Dunning-Kruger Effect:** This cognitive bias causes people with low ability in a domain to overestimate their ability. In cybersecurity, an employee who has completed basic awareness training may develop a false sense of confidence, believing they can easily spot any scam. This overconfidence makes them more likely to fall for sophisticated attacks that bypass their rudimentary heuristics.

These biases are not character flaws; they are features of the human operating system. An effective security programme must be designed with these features in mind, not in spite of them.

**The Environment Matters**

Even with the best intentions and an understanding of biases, behaviour is profoundly shaped by environment and context. A security culture cannot be built by focusing on the individual alone; we must also examine the world they operate within.

* **Friction vs. Flow:** Security is often a source of immense friction. It interrupts workflow, adds extra steps, and creates cognitive load. When secure behaviour is the harder path, even the most well-intentioned employee will eventually seek workarounds. If a secure file-sharing tool is slow and cumbersome, employees will revert to personal email. If password requirements are overly complex without a password manager, they will be written on sticky notes. The path of least resistance is a powerful force.  
* **Social Proof:** Humans are social animals who look to the behaviour of others to determine their own. If an employee sees their manager routinely bypassing security protocols (“just email that sensitive file to my personal account”), they receive a powerful message: security is not *really* important here. The formal policy is one thing, but the informal culture—‘the way things are really done’—is what dictates behaviour.  
* **Stress and Cognitive Load:** The modern workplace is an engine of stress and distraction. When an employee is juggling multiple urgent tasks, under pressure to meet a deadline, their cognitive resources are depleted. System 2, the deliberate mind, is switched off. They operate entirely on System 1 autopilot, making them highly vulnerable to manipulation and far more likely to take security shortcuts.

**Bridging the Divide: From Awareness to Behavioural Design**

Understanding these forces is the first step toward bridging the gap. The solution is not more awareness or louder warnings. It is a fundamental shift from *informing* employees to *designing* for them. We must move from security awareness to security behaviour and culture programs (SBCP) that are grounded in behavioural science.

This means:

1. **Reducing Friction:** Making the secure choice the easy choice. Integrating single sign-on (SSO) and password managers eliminates the friction of password fatigue. Simplifying approval processes for legitimate needs reduces the incentive to seek shadow IT workarounds.  
2. **Designing for System 1:** Using clear, intuitive cues and nudges. Instead of a text-heavy warning, a simple, colourful banner that clearly labels an external email provides a quick, System 1-friendly signal of potential risk.  
3. **Making it Social:** Leveraging social proof for good. Publicly celebrating teams and individuals who exemplify good security habits (“Security Champion of the Month”) makes secure behaviour visible and valued, encouraging others to follow suit.  
4. **Making it Relevant:** Combatting optimism bias by making threats personal and immediate. Using simulated phishing campaigns that are tailored to specific departments (e.g., a fake invoice for the finance team) makes the threat feel real and relevant, breaking down the “it won’t happen to me” barrier.

Bridging the great divide requires humility. It requires accepting that human behaviour is not a logic puzzle to be solved but a landscape to be shaped. It is the work of a designer, not a lecturer. It is about building guardrails on the path, not just giving someone a map and blaming them when they stumble. By accepting the realities of human psychology, we can finally stop lamenting the gap between knowing and doing and start building the structures to close it for good.

**References**

Kahneman, D. (2011). *Thinking, fast and slow*. Farrar, Straus and Giroux.

Sharot, T. (2011). The optimism bias. *Current Biology*, \*21\*(23), R941-R945.

Werlinger, R., Hawkey, K., & Beznosov, K. (2009). An integrated view of human, organizational, and technological challenges of IT security management. *Information Management & Computer Security*, \*17\*(1), 4-19.

Hadnagy, C. (2018). *Social Engineering: The Science of Human Hacking* (2nd ed.). John Wiley & Sons.

 

## 

## **2.2. The Fist and the Open Hand: Why Fear is a Poor Teacher**

There is an ancient method of training that relies on the fist: on force, on fear, on the threat of pain to compel obedience. It can produce a certain type of compliance, a flinching reaction to a specific command. But it cannot teach wisdom, adaptability, or genuine strength. It breaks the spirit to bend the will, creating a subject that obeys only when the master’s eye is watching, and resents in silence the rest of the time. The moment the threat is gone, so too is the motivation.

For years, the dominant paradigm in cybersecurity awareness has been the fist. We have attempted to bludgeon our employees into compliance through campaigns of fear. We have inundated them with terrifying statistics, graphic metaphors of burning cities and sinking ships, and dire warnings of personal culpability and termination. The underlying message has been clear: *“Be afraid. Make a mistake, and you will be the one who burns.”*

This chapter argues that this approach is not merely ineffective; it is actively counterproductive. Fear is a poor teacher and a corrosive foundation for a security culture. It creates a brittle, negative environment that drives the very behaviours we seek to eliminate. We will explore the psychological and organisational fallout of fear-based tactics and make the case for a different way: the open hand of empowerment, which builds a garden of trust where secure behaviours can truly take root and flourish.

**The Psychology of Fear**

Fear triggers a primal, biological response. It activates the amygdala, hijacking the prefrontal cortex—the part of the brain responsible for executive function, rational thought, and learning (LeDoux, 2000). In a state of fear, the body prepares for fight, flight, or freeze. This is the opposite of the state required for thoughtful decision-making and the adoption of new, complex skills.

When we use fear in security training, we are not educating; we are triggering a threat response. This has several destructive consequences:

* **Cognitive Impairment:** An employee who is afraid of making a mistake is not thinking clearly about the task at hand. Their cognitive resources are devoted to managing their anxiety, not to evaluating the authenticity of an email or the security of a decision. This ironically makes them *more* susceptible to the very threats we are warning them about, as their ability to critically analyse information is diminished.  
* **The Ostrich Effect:** Contrary to the intention, overwhelming fear often leads to avoidance and disengagement. When people feel a problem is too vast, too complex, or too terrifying, they psychologically disown it. They develop a learned helplessness, thinking, *“This is too big for me; I’ll just leave it to the experts,”* thereby abdicating their crucial role as the human sensor network. A study on risk perception found that high-fear messages about health risks often led to denial and avoidance rather than proactive behaviour (Witte, 1992).  
* **Compliance vs. Commitment:** Fear generates compliance, not commitment. An employee who is compliant will follow the rules only when they believe they are being monitored. They will not internalise the values behind the rules. They will not go the extra mile to report a near-miss or suggest a better process. Their engagement is transactional and minimal. Commitment, on the other hand, is driven by intrinsic motivation—a genuine belief in the importance of the goal.

**The Organisational Fallout**

The damage of a fear-based culture extends beyond individual psychology into the very architecture of the organisation.

* **The Rise of Shadow IT:** When security controls are perceived as oppressive barriers to productivity, employees will find creative workarounds. They will use unauthorized cloud storage, personal email, and messaging apps to get their work done efficiently. This “Shadow IT” creates an invisible, unmonitored, and massively vulnerable attack surface that completely bypasses all the security controls the organisation has painstakingly built. The fist of control begets the rebellion of shadow systems.  
* **The Death of Psychological Safety and the Birth of Silent Failure:** The single greatest cost of a fear-based culture is the death of psychological safety—the belief that one can speak up about mistakes, questions, or concerns without fear of punishment or humiliation (Edmondson, 2018). In a culture of blame, the absolute worst thing an employee can do is admit to clicking a phishing link or misconfiguring a setting. Therefore, they will hide it. This silent failure is a gift to attackers. It means intrusions can dwell in systems for months, spreading undetected because the very people who could sound the alarm are too afraid to speak up. The 2023 IBM Cost of a Data Breach Report consistently identifies that organisations with high levels of psychological safety and employee engagement have significantly lower breach costs, often by millions of dollars, precisely because issues are reported early.

**The Open Hand**

If the fist represents fear and control, the open hand represents empowerment and trust. It is a shift from being a punitive enforcer to being an enabling leader. The goal is not to force compliance, but to cultivate an environment where secure behaviour is the natural, and easiest, outcome.

This approach is grounded in the principles of intrinsic motivation and positive reinforcement. It focuses on:

* **Making Security Easy (Enablement):** Instead of blaming users for weak passwords, provide them with a seamless password manager. Instead of blocking file-sharing services, provide a sanctioned, user-friendly alternative that is more attractive than the shadow option. Reduce friction to make the right path the path of least resistance.  
* **Positive Reinforcement:** Celebrate desired behaviours. Instead of only punishing failures (e.g., shaming employees who fail phishing tests), actively reward and praise those who report phishing emails. Create a “Security Champion” program that recognises individuals and teams who exemplify good practices. This frames security as a positive, collaborative achievement, not a punitive obligation.  
* **Transparency and Trust:** Be open about the threat landscape and the organisation’s incidents. When leaders share stories of their own near-misses or what the organisation has learned from a past event, it normalises fallibility and creates a culture of shared learning. It signals that it is safe to talk about security failures because they are opportunities for improvement, not grounds for punishment.  
* **Focus on Coaching, Not Punishing:** Treat a security mistake not as a disciplinary issue, but as a training opportunity. A user who fails a phishing test should immediately receive just-in-time, constructive coaching on what to look for next time. This frames the organisation as invested in their growth and success, not in their punishment.

**Case in Point: The Phishing Training Paradigm Shift**

The evolution of phishing simulation programs perfectly illustrates this shift from the fist to the open hand.

* **The Fist (Old Model):** Mandatory simulations with a “gotcha” element. Employees who fail are automatically enrolled in punitive, mandatory re-training. Their names are sometimes highlighted on reports to managers. The result: anxiety, shame, and a culture of hiding failures.  
* **The Open Hand (New Model):** Simulations are framed as a collaborative learning tool, not a test. Those who report the phishing email are praised and thanked for their vigilance. Those who click receive immediate, constructive feedback in a non-shaming manner, often with a one-click option to view a short, engaging video explaining the red flags they missed. The result: a dramatic increase in reporting rates and a decrease in click-through rates, as employees feel empowered to be active participants in defence.

**From a Culture of Fear to a Culture of Vigilance**

The evidence is clear: fear might produce a flinch, but it will never build a fortress. It creates a brittle, silent, and ultimately vulnerable organisation. The fist of compliance seeks to minimise human error by attempting to remove the human from the equation. This is a fool’s errand.

The open hand of empowerment acknowledges that the human is the most critical part of the equation. It seeks to maximise human potential by creating an environment of trust, enablement, and positive reinforcement. It builds a resilient, adaptable, and vigilant culture where employees feel psychologically safe to be the first and best line of defence.

The choice for leaders is stark. We can continue to clench the fist, and watch as our people disengage and work around us. Or we can open our hand, offering the tools, trust, and training that empower them to stand with us. We can choose to rule through fear, or we can lead through inspiration. The former builds subjects who obey. The latter cultivates champions who defend.

 

**References**

Edmondson, A. C. (2018). *The fearless organization: Creating psychological safety in the workplace for learning, innovation, and growth*. John Wiley & Sons.

IBM Security. (2023). *Cost of a Data Breach Report 2023*. IBM Corporation.

LeDoux, J. E. (2000). Emotion circuits in the brain. *Annual Review of Neuroscience, 23*(1), 155-184.

Witte, K. (1992). Putting the fear back into fear appeals: The extended parallel process model. *Communication Monographs, 59*(4), 329-349.

 

 

## **2.3. The Three Stones of the Cooking Fire: A Foundation for Change**

In many traditions, the cooking fire is the heart of the home, the place where raw ingredients are transformed into nourishment for the community. Its strength and stability do not come from the flame alone, but from the three stones that hold the pot—each one essential, together creating an immutable foundation. Remove one stone, and the pot tips, the meal is lost, and the fire becomes a danger rather than a benefit.

For too long, our attempts to build a security culture have focused on stoking the flame—running louder awareness campaigns, issuing stricter policies, and amplifying threats. We have neglected the foundation. We have tried to hang the pot of behavioural change on a single, unstable stone of information and expected it to hold. It cannot.

True, lasting change requires a stable foundation. It rests on three indispensable pillars, three stones that must be laid with intention: 

1\.        **Enablement**   
2\.        **Persuasion**  
3\.        **Habit.** 

This chapter introduces this foundational model, a practical framework that will guide the rest of our journey. It moves beyond diagnosing why people fail and provides a clear, actionable blueprint for designing an environment where they can succeed.

**The First Stone: Enablement (Making the Secure Action the Easy Action)**

The first and most critical stone is Enablement. It is the principle that security must design for the human as they are, not as we wish them to be. We must make the secure path the path of least resistance. This stone addresses the fundamental truth of human nature: when faced with a choice between a difficult right way and an easy wrong way, context will almost always trump intention.

Enablement is the work of removing friction and providing tools. It is the antithesis of simply telling people what to do. It is about engineering the environment to guide behaviour effortlessly.

* **Technical Enablement:** This is the most direct form. It means providing seamless, integrated technology that makes secure behaviour effortless.  
  * **Example:** Enforcing complex password policies is a command. Providing a **single sign-on (SSO)** solution and an **enterprise password manager** that auto-generates and fills credentials is enablement. It completely removes the cognitive burden and time cost from the user.  
  * **Example:** Blocking all cloud storage is a command. Providing a **user-friendly, sanctioned file-sharing tool** that is faster and more efficient than the shadow alternative is enablement.  
* **Process Enablement:** Streamlining bureaucratic hurdles that force people to choose between security and productivity.  
  * **Example:** If requesting secure access to a needed resource takes a week and five forms, employees will find a workaround. Building a **self-service, automated access request portal** that provides approval in minutes is enablement. It aligns security with business agility.

When you enable effectively, you stop fighting human nature and start working with it. You preempt the creation of shadow IT and reduce the cognitive load on employees, freeing their mental resources for more complex decisions.

**The Second Stone: Persuasion (Speaking to the Heart and the Mind)**

The second stone is Persuasion. While Enablement makes the action easy, Persuasion provides the motivation to take it. This is where communication becomes strategic. It moves beyond broadcasting information to crafting messages that resonate deeply, leveraging the principles of behavioural science to influence attitudes and beliefs.

Persuasion understands that people are not purely rational actors; we are emotional beings influenced by social dynamics, identity, and narrative. Fear is a poor persuader; respect, social proof, and storytelling are powerful ones.

* **Framing and Messaging:** Instead of “Don’t click phishing links,” a persuasive message is, “Be a hero. Your vigilance is our first line of defence. Report suspicious emails and protect our team.” This frames the action positively, tying it to identity and shared purpose.  
* **The Power of Social Proof:** People look to the behaviour of others to guide their own. Persuasive campaigns leverage this.  
  * **Example:** “Join 85% of your colleagues in the Finance department who have enabled multi-factor authentication.” This is far more effective than a top-down mandate. Showcasing “Security Champions” and celebrating teams with best practices makes secure behaviour visible and aspirational.  
  * **Example:** A video message from a respected business leader (not the CISO) sharing why they value security protocols carries more weight than a hundred policy emails from IT.  
* **Storytelling and Relevance:** Abstract statistics about global breaches are easy to dismiss. A compelling story about a similar company that suffered a breach, told through the lens of the operational disruption and personal impact on employees, is unforgettable. Persuasion makes the threat tangible and personally relevant, combating optimism bias.

Persuasion is the art of connecting security to the values, identity, and social fabric of the organisation. It makes people *want* to be part of the solution.

**The Third Stone: Habit (Embedding Action into Unconscious Routine)**

The third stone is Habit. This is the ultimate goal: to make secure behaviour so ingrained that it becomes automatic, unconscious, and default. Habits require minimal cognitive effort; they are the brain’s way of saving energy. A strong security culture is ultimately a collection of good habits.

Habit formation is not magic; it is a science, often described as a loop: **Cue \> Routine \> Reward** (Duhigg, 2012). Our role is to design for this loop.

* **Designing Cues:** Integrate security prompts into natural workflows. A pop-up that reminds you to classify a document as it’s being saved is a cue. A browser extension that automatically flags external email addresses is a cue. These cues trigger the desired routine without the user having to remember.  
* **Supporting the Routine:** The routine is the behaviour itself—clicking “Report Phish,” using the password manager. This is where Enablement is crucial, ensuring the routine is simple and easy to execute.  
* **Providing the Reward:** This is the most overlooked step. The reward must be immediate and satisfying. For reporting a phishing email, the reward could be an instant, positive feedback message: *“Thank you\! You just helped protect our company.”* For completing training, it could be micro-badges or public recognition in a team channel. The reward reinforces the loop, making the user want to perform the routine again the next time they see the cue.

When these three elements align, behaviour shifts from a conscious effort to an automatic habit. Security becomes baked into the daily rhythm of work.

**The Interlocking Foundation: Why All Three Stones Are Non-Negotiable**

These three stones are not a menu to choose from. They are an interlocking system. Each one supports the others, and the structure collapses if one is missing.

* **Persuasion without Enablement** leads to frustration. You’ve convinced me of the importance of strong passwords, but you haven’t given me a tool to manage them. My motivation will quickly sour into resentment.  
* **Enablement without Persuasion** leads to low adoption. You’ve provided a fantastic password manager, but no one understands why they should use it instead of their old, easy method. The brilliant tool sits unused.  
* **Habit without Enablement and Persuasion** is impossible to form. You cannot create a sustainable habit if the behaviour is difficult (lack of enablement) or if the user doesn’t see its value (lack of persuasion).

Only when all three are present does transformation occur. Persuasion provides the *”why,”* Enablement provides the *”how,”* and Habit turns the action into *”what we do here.”*

**Building a Fire That Endures**

The Three Stones model provides a pragmatic, human-centric framework for leaders. It moves the conversation from “How do we force compliance?” to “How do we design for success?”

It challenges us to be architects and designers, not just policymakers. It demands that we invest not only in technology but in the careful, thoughtful construction of an environment that makes excellence easy, attractive, and routine.

By diligently laying these three stones—Enablement, Persuasion, and Habit—we build more than just a programme. We build a sustainable cooking fire at the heart of our organisation. A fire that does not rage and burn out, but that provides a steady, reliable heat. A fire that transforms the raw potential of our people into the nourishing strength of a resilient culture, capable of feeding and protecting the entire community for the long term.

**References**

Duhigg, C. (2012). The power of habit: Why we do what we do in life and business. Random House.  
Thaler, R. H., & Sunstein, C. R. (2008). Nudge: Improving decisions about health, wealth, and happiness. Yale University Press.  
Eyal, N. (2014). Hooked: How to build habit-forming products. Penguin.

 

## **2.4. From the Rainmaker's Visit to the Ever-Flowing Spring**

In times of drought, the village would call upon the rainmaker. This figure, often from afar, would arrive with ceremony and mystery, perform elaborate rituals, and, with luck, the skies would open. The village would celebrate, their immediate thirst quenched. But the rainmaker’s visit was a temporary spectacle. The water would soak into the earth and soon evaporate under the relentless sun. The underlying condition of the land remained unchanged—its soil depleted, its water table low. The village’s survival remained at the mercy of the next visit, the next performance, the next unpredictable downpour.

For decades, our approach to cybersecurity awareness has been that of the rainmaker. We descend upon the organisation annually or semi-annually with great fanfare: mandatory training campaigns, security awareness months, and high-profile phishing simulations. There is a flurry of activity. Metrics are generated—completion rates, click-through rates. We declare success, and then we depart. The “rain” of information falls, but it does not sink deep into the cultural soil. It is quickly forgotten, evaporated by the daily pressures of work, leaving the organisation’s behavioural landscape fundamentally unchanged and vulnerable to the next drought until the rainmaker’s next visit.

This chapter argues for a fundamental paradigm shift. We must abandon the hope of the occasional, dramatic downpour. Instead, we must become engineers of the ever-flowing spring. We must build a self-sustaining system where security is not an event, but an environment; not a periodic deluge, but a constant, reliable presence that nourishes the organisation continuously. This is the move from campaign-based awareness to culture-based engagement**.**

**The Limits of the Rainmaker: Why Campaigns Inevitably Fail**

The rainmaker model is seductive because it is simple, measurable, and fits neatly into a project management timeline. However, it is built on a flawed understanding of how humans learn, remember, and change behaviour.

* **The Forgetting Curve:** Hermann Ebbinghaus’s seminal work on memory demonstrates that humans forget an exponential amount of new information within hours or days of learning it if there is no reinforcement (Ebbinghaus, 1885/2013). A once-a-year training event is, from a cognitive perspective, almost entirely useless. By the time an employee encounters a real threat, the information from the training has long since faded.  
* **Lack of Context:** Rainmaker training is generic. It is designed for the “average” employee and delivered in a vacuum, disconnected from the specific workflows, tools, and pressures of an individual’s role. A phishing example about delivery notices might be relevant to an office admin but meaningless to a software developer. This lack of relevance accelerates the forgetting process and reinforces the idea that security is a separate, abstract concern, not integral to one’s actual job.  
* **The “Check-the-Box” Mentality:** This approach reduces security to a compliance exercise. Employees learn to complete the training to avoid a penalty from HR or IT, not to internalise the lessons. The goal becomes passing the test, not changing behaviour. This fosters cynicism and undermines the very message we are trying to convey.

The rainmaker provides a momentary spectacle, but the land remains parched. The organisation gains a certificate of completion, but not a more resilient culture.

**Engineering the Ever-Flowing Spring: Principles of Continuous Engagement**

Building a spring is harder, more nuanced work than hiring a rainmaker. It requires deep understanding of the landscape, constant maintenance, and a long-term perspective. It means embedding security into the very hydrology of the organisation—its communication channels, its rituals, its tools, and its leadership voice.

This is not a single project; it is a new operating model. It is built on three core principles:

**1\. Integration, Not Isolation:** Security messaging must flow through existing channels and rhythms of business, not through separate, segregated “security” channels.

* **Manager-Led Discussions:** Brief, regular (e.g., monthly) talking points on security are integrated into existing team meetings. When a line manager, not the CISO, discusses a recent threat or celebrates a team member for reporting something, it normalises security as a business leadership topic.  
* **Embedded Learning:** Micro-learning moments are embedded into the applications employees use daily. A five-second tooltip in the email client explaining how to spot a spoofed address. A quick checklist that appears when an employee goes to share a file externally. This is **just-in-time learning** that is directly relevant to the task at hand, defeating the forgetting curve.  
* **Collaboration with HR and Comms:** Security becomes a partner with Human Resources and Corporate Communications. Onboarding new hires includes a cultural welcome from a security champion, not just a mandatory module. Internal newsletters feature stories about how security enabled a business win.

**2\. Relevance and Context:** Messages are tailored to the audience. The spring provides different nourishment to different parts of the garden.

* **Role-Based Training:** The security guidance given to a developer (secure coding practices, dependency management) is different from that given to a finance officer (invoice fraud, wire transfer verification). Generic messages are replaced with highly specific, actionable guidance that employees immediately recognise as vital to their specific roles.  
* **Threat Intelligence Tailoring:** The security team translates global threat intelligence into actionable alerts for specific parts of the business. “Teams in Asia-Pacific, be aware of this new phishing tactic targeting our industry.” This makes threats feel real, immediate, and personal, combating optimism bias.

**3\. Dialogue, Not Monologue:** The rainmaker speaks; the spring listens and responds. A culture of engagement is a two-way conversation.

* **Friction Logging:** Create simple channels for employees to report security frustrations (e.g., “This secure process is too slow and I had to find a workaround”). This is not treated as complaining, but as invaluable feedback for the security team to improve enablement. It turns employees into co-designers of the security environment.  
* **Gamification and Positive Reinforcement:** Use platforms that allow for positive competition and recognition. Leaderboards for departments with the most phishing reports, badges for completing micro-learning challenges, and public recognition from leadership for secure behaviours. This taps into intrinsic motivators like status, autonomy, and mastery (Pink, 2009).

**The Role of Leadership: Stewards of the Spring**

This shift cannot be delegated. It requires leaders to become stewards of the spring, not just sponsors of the rainmaker.

* **Modelling Behaviour:** Leaders must visibly and consistently model the behaviours they expect. When they use password managers, report suspicious emails, and speak openly about security in business terms, they send a powerful message that this is valued.  
* **Providing Resources:** Building and maintaining a spring requires investment—not just in technology, but in the time and personnel needed to create tailored content, manage feedback channels, and foster continuous engagement. Leaders must recognise this not as an overhead, but as a critical investment in cultural infrastructure.  
* **Measuring the Right Things:** We stop measuring “training completion rates” and start measuring indicators of cultural health: **number of phishing reports submitted, number of friction logs filed, reduction in shadow IT usage, employee sentiment on security surveys.** These metrics tell us about the depth of the water table, not just the intensity of the last storm.

**Nourishment for the Long Journey**

The journey from the rainmaker’s visit to the ever-flowing spring is a journey from spectacle to sustainability, from compliance to commitment, from fear to empowerment.

The rainmaker offers a temporary solution for a chronic problem. The spring represents a permanent source of resilience. It ensures that the organisation is continuously nourished by a steady flow of relevant, contextual, and engaging security wisdom. It weaves security into the daily fabric of work until it becomes indistinguishable from competence itself.

This is how we finally bridge the great divide between knowing and doing. Not with a dramatic once-a-year performance, but with the constant, reliable presence of an ever-flowing spring. We stop trying to scare people into remembering and start building a world where secure behaviour is the easiest, most natural, and most rewarding choice, every single day. The drought is over. The work of cultivation begins.

 

**References**

Ebbinghaus, H. (2013). *Memory: A contribution to experimental psychology*. (H. A. Ruger & C. E. Bussenius, Trans.). Annals of Neurosciences, 20(4), 155–156. (Original work published 1885\)

Pink, D. H. (2009). *Drive: The surprising truth about what motivates us*. Riverhead Books.

Kahneman, D. (2011). *Thinking, fast and slow*. Farrar, Straus and Giroux.

 

## **2.5. A Tale of Empowerment**

There is an old story about phishing that we have told for too long. It is a story of fear and blame. In this story, the employee is a foolish fish, swimming mindlessly in the digital ocean. The hacker is a cunning angler, dangling a tempting hook. The security team are the frustrated fishermen on the shore, watching in dismay as their colleagues, one after another, take the bait. The moral of this story is clear: the fish is the problem. The solution is to train the fish to be smarter, to yell at the fish when it fails, and to hope that fear will make it more wary next time.

This story has shaped our entire approach. It has given us punitive phishing simulations designed to “test” and “catch” people. It has created cultures of shame where employees hide their mistakes. It has turned the security team into enforcers and the workforce into a problem to be managed. This story has failed us.

It is time to tell a new story. This is not a story about stupid fish and clever anglers. It is a story about a school of fish, evolving together. It is a story where the wisest fish, the one who spots the hook, does not simply swim away. It alerts the entire school, and together, they learn the angler’s tricks, they protect the vulnerable among them, and they become collectively smarter and stronger. This is a story of collective intelligence, not individual failure. This is a story of empowerment.

This chapter is about that new story. It is a practical case study in how to apply the Three Stones—Enablement, Persuasion, and Habit—to transform one of the most pervasive security challenges from a source of friction into a foundation of strength.

**The Old Story: The Psychology of the “Gotcha”**

The traditional phishing simulation is a weaponised version of the old story. Its primary design principle is to trick people. It uses increasingly sophisticated lures to see who will click. When an employee fails, the consequence is often automatic, punitive re-training, and their name may be highlighted on a report to their manager.

The psychological impact of this “gotcha” model is devastating:

* **It Breeds Fear and Anxiety:** Employees come to see the security team as an adversary trying to entrap them, not as allies trying to protect them.  
* **It Encourages Hiding Mistakes:** If the consequence of clicking a test phishing email is shame and mandatory punishment, the logical response to a *real* phishing email is to hide the click, not report it. This destroys our early warning system.  
* **It Fosters Resentment:** Employees feel they are being tested on a skill they were never properly taught, in a high-pressure environment where a moment’s distraction leads to public failure.

This model measures failure, but it does not create security. It is a perfect example of the fist, and it creates exactly the behaviours we seek to avoid.

**The New Story: The Three Stones in Action**

The new story dismantles the “gotcha” model and rebuilds it around empowerment, using our three foundational principles.

**1\. Enablement: The One-Click Shield**  
The first step is to make reporting effortless. The single most important action an employee can take is not to avoid clicking, but to report a suspicious email. We must enable this.

* **The Tool:** Every email client must have a single, highly visible “Report Phish” button. This button should not generate a complex ticket; it should send the email directly to the security operations centre with one click.  
* **The Process:** The backend process must be seamless. The security team must have a playbook to quickly analyse reported emails, provide feedback to the user, and use the data to improve filters. This closes the loop and shows employees their action has value.

Enablement turns a complex cognitive decision:  
 (“Is this phishing? What should I do? Who should I call?”) into a simple, frictionless action (“I’m not sure, I’ll just report it”).

**2\. Persuasion: Reframing the Hero’s Journey**  
We must change the narrative around reporting. This is a shift from negative reinforcement to positive reinforcement.

* **From “Don’t Get Caught” to “Be a Hero”:** The messaging around phishing must celebrate reporters, not punish clickers. Communications should say: “You are our first line of defence. Your eyes on the front lines are our most valuable sensor. When you report, you are a hero who protects our team.”  
* **Instant Positive Reinforcement:** The moment an employee clicks “Report Phish,” they should receive immediate, positive feedback. An automated message could pop up: *“Thank you for your vigilance\! The security team has received your report and will investigate.”* This simple act of gratitude is a powerful reward that reinforces the desired behaviour.  
* **Social Proof and Recognition:** Publicly celebrate individuals and teams with the highest report rates. Create a “Phish Spotter of the Month” award. Share stories in company newsletters about how an employee’s report stopped a real attack. This leverages social proof to make reporting an aspirational, valued behaviour.

**3\. Habit: Building the Muscle Memory of Vigilance**  
The goal is to make reporting an automatic, habitual response to anything suspicious.

* **The Cue:** The strange email itself is the cue. The subject line, the unfamiliar sender, the sense of urgency—these all trigger the feeling of suspicion.  
* **The Routine:** The new, ingrained routine must be: feel suspicion \-\> click “Report Phish.” This is where seamless enablement is critical. The routine must be easier than forwarding the email, easier than deleting it, and certainly easier than ignoring it.  
* **The Reward:** The immediate “thank you” message and the broader culture of recognition provide the reward that cements the habit loop. The employee feels a sense of efficacy and contribution, making them want to repeat the action.

**Measuring What Matters**

When you implement this new model, your metrics for success fundamentally change. You stop measuring **failure rates** (the percentage of people who click) and start measuring **engagement rates** (the percentage of people who report).

A high number of reports is a sign of a healthy, engaged human sensor network. It means people are paying attention and are not afraid to participate. Even if click rates remain initially unchanged, a skyrocketing report rate is a victory. It means you have visibility. You can now see the attacks coming in and can use that data to:

* **Improve Technical Controls:** Automatically add reported senders to blocklists and use the emails to train AI-based filters.  
* **Provide Targeted Coaching:** For the employee who clicks, the response is not punishment. It is immediate, constructive, and kind coaching. A short, engaging video can play right after the click, explaining the specific red flags in that email. This is just-in-time learning that is far more effective than annual training.  
* **Build a Learning Culture:** Every reported email becomes a data point for the entire organisation. The security team can send out “Phish of the Week” alerts, showing a real example that was reported and explaining why it was suspicious. This continuous, relevant feedback makes the entire school of fish smarter.

**The School Grows Stronger**

The new story of the phishing hook is a story of cultural transformation. It is a move from a model of individual blame to one of collective defence. It demonstrates that when we stop trying to fix people and start trying to empower them, we unlock incredible potential.

The empowered employee is not a perfectly vigilant automaton who never makes a mistake. They are a engaged human being who is occasionally tricked, but who is never afraid to raise their hand and say, “I see something.” They are the wisest fish, not because they are immune to hooks, but because they know that their strength lies in the school.

By applying the principles of Enablement, Persuasion, and Habit, we stop throwing hooks at our own people. Instead, we give them the tools, the motivation, and the practice to spot the real anglers in the water. We build an organisation where security is not a solitary test of individual willpower, but a collaborative triumph of collective intelligence. We stop telling tales of failure, and we start building a legend of resilience.

 

**References**

Duhigg, C. (2012). *The power of habit: Why we do what we do in life and business*. Random House.

Hadnagy, C. (2018). *Social Engineering: The Science of Human Hacking* (2nd ed.). John Wiley & Sons.

Kahneman, D. (2011). *Thinking, fast and slow*. Farrar, Straus and Giroux.

 

## **2.6 Preparing the Soil for Planting**

We have walked a significant path together in this chapter. We began by acknowledging the empty feast of awareness—the spectacle that fails to nourish. We stared into the great divide between knowing and doing, understanding the cognitive and environmental currents that pull intention and action apart. We challenged the clenched fist of fear, recognising its power to compel compliance but its utter failure to build committed, resilient vigilance.  
From this clearing of the old ground, we laid a new foundation: the Three Stones of Enablement, Persuasion, and Habit. This model provides a robust, human-centric framework for designing an environment where secure behaviour can flourish. We then envisioned the shift from the rainmaker’s dramatic, fleeting visit to the steady, life-giving flow of the ever-flowing spring, where security is integrated, continuous, and relevant. Finally, we witnessed this philosophy in action, rewriting the story of the phishing hook from a tale of blame into one of collective empowerment.  
This journey has moved us from the why of behavioural change to the how. We are no longer merely diagnosing an illness; we are holding the scalpel and the sutures, ready to operate. We possess the principles, the models, and the illustrative examples. The temptation now, the impulsive urge, is to charge forward—to immediately launch a new tool, a new campaign, a new series of trainings based on this new understanding.  
But the wise farmer does not scatter precious seed onto untilled, unassessed earth. They first test the soil. They study its composition, its pH, its moisture. They understand its unique strengths and deficiencies. To do otherwise is to waste seed and guarantee a poor harvest.  
So it is with our organisational culture. The principles we have explored are the seeds of transformation. But they cannot be sown blindly. Your organisation’s cultural soil is unique. Its composition is a complex blend of historical experiences, leadership styles, departmental subcultures, and deeply ingrained habits—both good and bad. What grows brilliantly in one organisation may wither in another due to unobserved cultural conditions.  
Therefore, this moment of conclusion is not an end, but a purposeful pause. It is the critical transition from theory to deliberate practice. The work that lies immediately ahead is not action, but diagnosis. Before we can build, we must listen. Before we can change, we must understand.

The next chapter, “Diagnosing Your Current Culture,” is our soil test. It is the essential, often overlooked step that separates successful, sustained transformation from wasteful, superficial initiative. It provides the tools to move beyond assumptions and anecdotes, giving you an evidence-based understanding of your cultural landscape.

In it, we will explore how to:

* Listen to the whispers and shouts of your culture through surveys, focus groups, and friction logs, moving beyond what people say they do to discover what they actually do.  
* Map the informal networks of influence that truly shape behaviour, identifying your potential champions and understanding the hidden barriers to change.  
* Measure the current state not with punitive metrics, but with compassionate curiosity, establishing a baseline from which progress can be truly measured.

This diagnostic phase is an act of profound respect for your organisation and its people. It signals that you are not imposing a pre-packaged solution, but are instead engaging in a collaborative process of discovery and co-creation. It builds the trust necessary for the changes to come.

The seeds of Enablement, Persuasion, and Habit are ready. The blueprint for the ever-flowing spring is drawn. But first, we must pick up the tools of the diagnostician. We must prepare the soil. For only from ground that is deeply understood, carefully cleared, and thoughtfully nourished can a truly transformative culture—one that is resilient, empowered, and enduring—take root and grow.

The work of planting awaits. But first, we must test the earth.  
   
**References**  
Schein, E. H. (2010). Organizational culture and leadership (4th ed.). Jossey-Bass.  
Weiner, B. J. (2009). A theory of organizational readiness for change. Implementation Science, 4(1), 67\.
